Based on the evaluation of the NER model trained using the stanford-wikipedia-training.txt file and evaluated with wikipedia-gold.txt, the precision, recall, and F1-score provide insights into the model's performance. 
Precision measures the accuracy of the predicted entities, while recall assesses how well the model captures all relevant entities, with the F1-score balancing these two metrics. 
The results show high precision (0.88) and recall (0.9167) for LOCATION entities, resulting in a strong F1-score of 0.8980, indicating effectiveness in identifying locations.
However, the model struggles with ORGANIZATION entities, achieving a precision of 0.7353 and recall of 0.4854, leading to a lower F1-score of 0.5848. 
This could be due to misidentification of less common organizations such as "WorldCat," "Scientology," and "Rogers and Cowan." 
PERSON entities showed high precision (0.9190) but lower recall (0.6558), resulting in an F1-score of 0.7654. 
This discrepancy is partly because the NER model correctly identified full names like "Tom Cruise" as PERSON but often missed tagging the surname "Cruise" alone as PERSON, leading to more false negatives and a lower recall.


The evaluation on the fanwiki dataset highlights significant challenges, particularly in identifying ORGANIZATION and PERSON entities. 
The model failed entirely to recognize any ORGANIZATION entities, with both precision and recall at 0, often misclassifying names like "Ivo De Palma" and "Enya" as PERSON instead of ORGANIZATION. 
For LOCATION entities, the model achieved perfect scores, likely due to the straightforward nature of the few instances, such as "Egypt" and "Atlantic." 
For PERSON entities, the model showed high precision (0.9744) but a lower recall (0.5135), resulting in an F1-score of 0.6726, reflecting difficulties in detecting Japanese names common in JoJo's Bizarre Adventure. 
The overall F1-score of 0.5797 highlights the need for better differentiation between entity types within the fan-related context.


To address potential issues with punctuation tagging errors, I used fix-ner.py to correct punctuation tags to always be /O. 
This script was applied to both the standard-wikipedia-training.txt and standard-fanwiki-training.txt files, generating corrected versions. 
After re-running the training using standard-wikipedia-training-corrected.txt and standard-fanwiki-training-corrected.txt and 
testing on their respective gold standard files (wikipedia-gold.txt and fanwiki-gold.txt), the model's performance did not show significant improvement. 
This lack of impact is likely because the original datasets already had minimal punctuation tagging errors, with only a few instances needing correction. 
As a result, these minor fixes did not meaningfully alter the overall performance metrics.

